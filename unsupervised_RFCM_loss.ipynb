{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Silence tensorflow warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check that tf is running on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define constants/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 512, 512\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 10\n",
    "FILTERS = [32, 64, 128, 256]\n",
    "path_to_images = '<path to images>'\n",
    "# CLASS_WEIGHTS = [0.018132846245274156, 0.9198221511503452, 0.06204500260438064]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the custom loss function class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFCM_loss():\n",
    "\n",
    "    def __init__(self, fuzzy_factor=2):\n",
    "        '''\n",
    "        Unsupervised Robust Fuzzy C-mean loss function for ConvNet based image segmentation\n",
    "        Junyu Chen, et al. Learning Fuzzy Clustering for SPECT/CT Segmentation\n",
    "        via Convolutional Neural Networks. Medical physics, 2021 (In press).\n",
    "        :param fuzzy_factor: exponent for controlling fuzzy overlap, default value = 2\n",
    "        :param regularizer_wt: weighting parameter for regularization, default value = 0\n",
    "        Note that ground truth segmentation is NOT needed in this loss fuction, instead, the input image is required.\n",
    "        :param y_pred: prediction from ConvNet, assuming that SoftMax has been applied.\n",
    "        :param image: input image to the ConvNet.\n",
    "        '''\n",
    "        self.fuzzy_factor = fuzzy_factor\n",
    "\n",
    "    def rfcm_loss_func(self, image, y_pred):\n",
    "        img_channels = 3\n",
    "        # num_clus = y_pred.get_shape().as_list()[-1]  # num_clus equals the classes of the problem\n",
    "        num_clus = y_pred.shape[-1]  # num_clus equals the classes of the problem\n",
    "\n",
    "        # flatten y_true(img) and prediction (y_pred)\n",
    "        img = tf.reshape(image, (-1, tf.reduce_prod(tf.shape(image)[1:-1]), img_channels))\n",
    "        # y_pred/seg represents the initial random probability assignments\n",
    "        seg = tf.reshape(y_pred, (-1, tf.reduce_prod(tf.shape(y_pred)[1:-1]), num_clus))\n",
    "        J_1 = 0\n",
    "        for i in range(num_clus):\n",
    "            J_2 = 0\n",
    "            # mem --> membership value: sum_square of probs assigned \n",
    "            mem = tf.pow(seg[..., i], self.fuzzy_factor)\n",
    "            for j in range(3):\n",
    "                img_channel = img[..., j]\n",
    "                # calculate centroid\n",
    "                centroid = tf.reduce_sum(tf.multiply(img_channel, mem))/tf.reduce_sum(mem)\n",
    "                # calculate distances from centroid\n",
    "                J_2 += tf.multiply(mem, tf.square(img_channel - centroid))\n",
    "            J_1 += J_2 / num_clus\n",
    "        return tf.reduce_mean(J_1)\n",
    "        \n",
    "\n",
    "    def loss(self, I, J):\n",
    "        return self.rfcm_loss_func(I, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(path_to_images, fname)\n",
    "        for fname in os.listdir(path_to_images)\n",
    "        if fname.endswith((\".JPG\", \".png\"))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create helper class to create dataset/batches from input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(tf.keras.utils.Sequence):\n",
    "\n",
    "    \"\"\"Helper to create datasets\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size)\n",
    "            x[j] = img\n",
    "            x[j] /= 255\n",
    "        return x, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = CustomDataset(\n",
    "    BATCH_SIZE,\n",
    "    IMG_SIZE,\n",
    "    input_img_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The encoder part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(inputs, filters_list, size=3, apply_dropout=False):\n",
    "\n",
    "  encoder_outputs = []\n",
    "  init_inputs = inputs  \n",
    "  # initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  for filters in filters_list:\n",
    "\n",
    "    x = layers.SeparableConv2D(filters,\n",
    "                               size,\n",
    "                               padding='same',\n",
    "                               # kernel_initializer=initializer,\n",
    "                               use_bias=False)(inputs)\n",
    "    if apply_dropout:\n",
    "      x = layers.Dropout(0.5)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.SeparableConv2D(filters,\n",
    "                               size,\n",
    "                               padding='same',\n",
    "                               # kernel_initializer=initializer,\n",
    "                               use_bias=False)(x)                           \n",
    "    x= layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "    if apply_dropout:\n",
    "      x = layers.Dropout(0.5)(x)\n",
    "    x = layers.ReLU(name=f'activation_{filters}')(x)\n",
    "    x = layers.BatchNormalization()(x)  \n",
    "\n",
    "    encoder_outputs.append(x)\n",
    "    inputs = x\n",
    "\n",
    "  return tf.keras.Model(inputs=init_inputs, outputs=encoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The decoder part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decode_layer(filters, size=3, apply_dropout=False):\n",
    "  \n",
    "  # initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(layers.Conv2DTranspose(filters, size,\n",
    "                                              padding='same',\n",
    "                                              # kernel_initializer=initializer,\n",
    "                                              use_bias=False))\n",
    "  result.add(layers.Conv2DTranspose(filters, size,\n",
    "                                            padding='same',\n",
    "                                            # kernel_initializer=initializer,\n",
    "                                            use_bias=False))\n",
    "\n",
    "  result.add(layers.UpSampling2D(2))\n",
    "  result.add(layers.ReLU())\n",
    "  result.add(layers.BatchNormalization())                                         \n",
    "  if apply_dropout:\n",
    "    result.add(layers.Dropout(0.5))  \n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(input_shape, filters_list, output_channels:int, activation:str):\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape + (3,))\n",
    "\n",
    "    layer_names = [f'activation_{filters}' for filters in filters_list]\n",
    "\n",
    "    # encoder part of the model\n",
    "    encoder = create_encoder(inputs, filters_list)\n",
    "    # get encoder_outputs\n",
    "    skips = [encoder.get_layer(name).output for name in layer_names]\n",
    "    # decoder part of the model\n",
    "    decoding_layers = [get_decode_layer(filters) for filters in filters_list[::-1]]\n",
    "\n",
    "    x = skips[-1]\n",
    "    # reverse order!\n",
    "    skips = skips[-2::-1]\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up_layer, skip in zip(decoding_layers[:-1], skips):\n",
    "        x = up_layer(x)\n",
    "        concat = layers.Concatenate()\n",
    "        x = concat([x, skip])\n",
    "\n",
    "    x = decoding_layers[-1](x)\n",
    "    # last layer of the model\n",
    "    last_layer = layers.Conv2D(filters=output_channels, kernel_size=1,\n",
    "                                            padding='same',\n",
    "                                            activation=activation\n",
    "                                        ) \n",
    "\n",
    "    x = last_layer(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = get_unet(\n",
    "    IMG_SIZE,\n",
    "    FILTERS,\n",
    "    NUM_CLASSES,\n",
    "    activation='softmax'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model.compile(\n",
    "                optimizer='adam',\n",
    "                # loss = NCutLoss2D().ncutloss_func,\n",
    "                loss=RFCM_loss().rfcm_loss_func,\n",
    "                # metrics=[tf.keras.metrics.SparseCategoricalAccuracy(), MyMeanIOU(NUM_CLASSES) ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_02 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    './sem_seg_model_backbone_4cl',\n",
    "    monitor=\"loss\",\n",
    "    mode = 'min',\n",
    "    save_best_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "unet_model.fit(\n",
    "    train_gen,\n",
    "    epochs=epochs,\n",
    "    callbacks=[callback_02]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper function to visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_superimposed_img(path, num_clusters, model):\n",
    "    \n",
    "    random_image = load_img(path, target_size=IMG_SIZE)\n",
    "    rand_arr = img_to_array(random_image)\n",
    "    rand_arr /= 255\n",
    "    rand_arr = tf.expand_dims(rand_arr, 0)\n",
    "    preds = model.predict(rand_arr)\n",
    "    mask = tf.argmax(preds, axis=-1)\n",
    "    mask = tf.squeeze(mask)\n",
    "    # print(np.unique(mask, return_counts=True))\n",
    "    \n",
    "    segm_img = np.zeros(shape=IMG_SIZE+(3,))\n",
    "    \n",
    "    for clus in range(num_clusters):\n",
    "        for i in range(3):\n",
    "            mask_r = np.where(mask == clus, np.random.choice(255), 0)\n",
    "            mask_g = np.where(mask == clus, np.random.choice(255), 0)\n",
    "            mask_b = np.where(mask == clus, np.random.choice(255), 0)\n",
    "            stacked_mask = np.stack((mask_r, mask_g, mask_b), axis=2)\n",
    "        segm_img +=stacked_mask    \n",
    "    # superimposed_img = random_image + segm_img\n",
    "\n",
    "    return array_to_img(segm_img)\n",
    "    # return tf.keras.preprocessing.image.array_to_img(superimposed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_superimposed_img(random.choice(input_img_paths), NUM_CLASSES, tf.keras.models.load_model('<path to your model>', custom_objects={'rfcm_loss_func': RFCM_loss.rfcm_loss_func}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('python3.8_tf2.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "275673c95fae1bad921d2741a5164e2bd11f2dfbf960e57d3d382ba482819447"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
